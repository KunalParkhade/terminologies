# Stochastic Gradient Descent
- The gradient is a vector that tells us in what direction the weights need to go. 
- More precisely, it tells us how to change the weights to make the loss change fastest. 
- We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. 
- Stochastic means "determined by chance." Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called **SGD**! 
